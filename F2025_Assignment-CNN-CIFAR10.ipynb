{"cells":[{"cell_type":"markdown","metadata":{"cellIdentifier":"gxd46v5852dxd94btcq4ba","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"Azp1bdf4iIdi"},"source":["# <font style=\"color:blue\">Assignment: Implement a CNN for Image Classification on CIFAR10 dataset</font>\n","\n","We have seen how to implement a CNN (LeNet5 and LeNet with the batch norm) in the last section. We used MNIST and Fashion MNIST dataset which are grayscale or single channel datasets. In this assignment, you will implement a CNN Model ( similar to LeNet ) for classifying objects in the `CIFAR10` dataset.\n","\n","The CIFAR10 dataset has the following properties\n","1. It has `10` classes.  \n","1. It has colored images, so it has `3-channels`.\n","1. The image shape is `32 x 32`.\n","\n","Samples of CIFAR10- dataset ([source](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html?highlight=cifar)):\n","\n","<img src=\"https://www.learnopencv.com/wp-content/uploads/2020/01/c3_w3_cirar10.png\" width=700>\n","\n","\n","# <font color='blue'>Marking Scheme</font>\n","\n","### <font style=\"color:green\">Maximum Points: 30\n","\n","<div>\n","    <table>\n","        <tr><td><h3>Sr. no.</h3></td> <td><h3>Problem</h3></td> <td><h3>Points</h3></td> </tr>\n","        <tr><td><h3>1</h3></td> <td><h3>Implement the CNN Model</h3></td> <td><h3>10</h3></td> </tr>\n","        <tr><td><h3>2</h3></td> <td><h3>Find Mean and Std of Training Data</h3></td> <td><h3>5</h3></td> </tr>\n","        <tr><td><h3>3</h3></td> <td><h3>Model Training & Accuracy</h3></td> <td><h3>15</h3></td> </tr>\n","    </table>\n","</div>\n","\n","\n","# <font color='blue'>Problem Description</font>\n","\n","### <font color='blue'>1. Implement the CNN Model</font>\n","Since the task is to classify objects in a dataset of color images, you need to implement a CNN with 10 output classes. **Also, your model must use `Conv2d`, `BatchNorm2d`, and `ReLU`.**\n","\n","**You need to define the model architecture in the function: `MyModel` ( Step 1 )**\n","\n","Hint: For color images you need to use an input shape that is different than the ones we have been using till now, so that it accepts 3 channel inputs.\n","\n","### <font color='blue'>2. Find Mean and Std of Training Data</font>\n","\n","It is a good practice to normalize the training data. To normalize the data, we need to compute mean and std. As the dataset has colored images, it has `3-channel` (RGB or BGR). We have to find mean and std per channel using training data.\n","\n","**You need to compute the mean and std for the dataset in the function: `get_mean_std_train_data` ( Step 3 )**\n","\n","### <font color='blue'>3. Model Training and Accuracy</font>\n","\n","Once you have defined the model, you can train it. To get better accuracy, you need to play around the training configuration **( Step 5 )** and even the model architecture. You can check the accuracy by running the training loop in `Step 11`.\n","\n","Here are a few hints on how you can improve the accuracy:\n","- Train for longer duration\n","- Try with different learning rate\n","- Try to add more convolutional layers to the architecture\n","- Try to add more nodes in the layers.\n","\n","You need to achieve **75% accuracy** ( See Step11 ) in order to get full marks for this part.\n","\n","**You do not need to implement anything for this, just changing the parameters as mentioned above and running the Notebook will give you the accuracy. ( Step 5 and Step 11 )**"]},{"cell_type":"markdown","metadata":{"cellIdentifier":"v569wpwjlfhrpxsb1armx8","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"LalyjY4aiIdk"},"source":["Note that this notebook requires you to change a few stuff in the model to get the desired accuracy. Therefore you need train the model all over again (which seems to be time-consuming).   \n","\n","You can choose to execute this notebook in Google-Colab so that you have access to a GPU-machine and prototype faster.   \n","\n","Once the desired results are acheived, you can copy-paste the changes made in the Colab-notebook to this notebook so that the grading occurs on the latest code. We recommend to download your trained model checkpoint and upload with filename `cifar10_cnn_model.pt` under `models` directory in the labs.\n","\n","You can access the Colab-notebook from [here](https://colab.research.google.com/drive/18lgSRmHPagJkB0xmDq5ZiWGkuWcHnUc2?usp=sharing)"]},{"cell_type":"markdown","source":[],"metadata":{"id":"M7lilQXTkBQe"}},{"cell_type":"markdown","metadata":{"cellIdentifier":"v569wpwjlfhrpxsb1armx8","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"xUX80zqGkBvB"},"source":["Note that this notebook requires you to change a few stuff in the model to get the desired accuracy. Therefore you need train the model all over again (which seems to be time-consuming).   \n","\n","You can choose to execute this notebook in Google-Colab so that you have access to a GPU-machine and prototype faster.   \n","\n","Once the desired results are acheived, you can copy-paste the changes made in the Colab-notebook to this notebook so that the grading occurs on the latest code. We recommend to download your trained model checkpoint and upload with filename `cifar10_cnn_model.pt` under `models` directory in the labs.\n","\n","You can access the Colab-notebook from [here](https://colab.research.google.com/drive/18lgSRmHPagJkB0xmDq5ZiWGkuWcHnUc2?usp=sharing)"]},{"cell_type":"code","execution_count":111,"metadata":{"cellIdentifier":"s0qa9hxkttgzsi91m1gxb","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"dNjffiXNiIdl","executionInfo":{"status":"ok","timestamp":1761597021084,"user_tz":300,"elapsed":3,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":["required_training = True"]},{"cell_type":"code","execution_count":112,"metadata":{"cellIdentifier":"hyxuazrgf57wbylo94w0l","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"V6TurvTniIdm","executionInfo":{"status":"ok","timestamp":1761597021089,"user_tz":300,"elapsed":2,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"code","execution_count":113,"metadata":{"cellIdentifier":"4ha2g9gk1fwp7h5qpbzwtf","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"l9UOPm5OiIdm","executionInfo":{"status":"ok","timestamp":1761597021098,"user_tz":300,"elapsed":7,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":["import matplotlib.pyplot as plt  # one of the best graphics library for python"]},{"cell_type":"code","execution_count":114,"metadata":{"cellIdentifier":"coj3psnkaukhlx2xlfkt5p","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"d5dPYBFOiIdn","executionInfo":{"status":"ok","timestamp":1761597021099,"user_tz":300,"elapsed":0,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":["import os\n","import time\n","\n","from typing import Iterable\n","from dataclasses import dataclass\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","from torchvision import datasets, transforms\n","import torchvision"]},{"cell_type":"markdown","metadata":{"cellIdentifier":"8wph086jktcmkl2r8ghlzc","id":"rAdHmR4miIdn"},"source":["# <font style=\"color:blue\">1. CNN Model Architecture [30 Points]</font>\n","\n","You have to write the model code here. I have provided an example of the LeNet Model for your reference that you can edit. In general, it is good practice to break your system into pieces. In this case, the self._body piece defines the feature detection piece of the system and the self._head defines the classifier piece. Finally, forward defines the forward pass.\n","\n","Notes on what to change in the model, you will need to change the input so that it works with color images with three input channels.\n","\n","Your model should achieve at least 75% accuracy. If you do not get higher accuracy, here are a few hints:\n","- Try to add more convolutional layers to the architecture\n","- Try to add more nodes in the layers.\n"]},{"cell_type":"code","execution_count":115,"metadata":{"cellIdentifier":"0az2d4cocj0in9bvr6riiee","id":"bBbrE7IKiIdo","executionInfo":{"status":"ok","timestamp":1761597021108,"user_tz":300,"elapsed":3,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":["class LeNet5(nn.Module):\n","    def __init__(self, num_classes, batch_norm=False):\n","        super().__init__()\n","\n","        # convolution layers\n","\n","        if batch_norm:\n","            self._body = nn.Sequential(\n","                # First convolution Layer\n","                # input size = (32, 32), output size = (28, 28)\n","                nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n","                nn.BatchNorm2d(6),\n","                # ReLU activation\n","                nn.ReLU(inplace=True),\n","                # Max pool 2-d\n","                nn.MaxPool2d(kernel_size=2),\n","\n","                # Second convolution layer\n","                # input size = (14, 14), output size = (10, 10)\n","                nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n","                nn.BatchNorm2d(16),\n","                nn.ReLU(inplace=True),\n","                nn.MaxPool2d(kernel_size=2),\n","                # output size = (5, 5)\n","            )\n","\n","\n","        else:\n","            self._body = nn.Sequential(\n","                # First convolution Layer\n","                # input size = (32, 32), output size = (28, 28)\n","                nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n","                # ReLU activation\n","                nn.ReLU(inplace=True),\n","                # Max pool 2-d\n","                nn.MaxPool2d(kernel_size=2),\n","\n","                # Second convolution layer\n","                # input size = (14, 14), output size = (10, 10)\n","                nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n","                nn.ReLU(inplace=True),\n","                nn.MaxPool2d(kernel_size=2),\n","                # output size = (5, 5)\n","            )\n","\n","        # Fully connected layers\n","        self._head = nn.Sequential(\n","            # First fully connected layer\n","            # in_features = total number of weights in last conv layer = 16 * 5 * 5\n","            nn.Linear(in_features=16 * 5 * 5, out_features=120),\n","\n","            # ReLU activation\n","            nn.ReLU(inplace=True),\n","\n","            # second fully connected layer\n","            # in_features = output of last linear layer = 120\n","            nn.Linear(in_features=120, out_features=84),\n","\n","            # ReLU activation\n","            nn.ReLU(inplace=True),\n","\n","            # Third fully connected layer. It is also output layer\n","            # in_features = output of last linear layer = 84\n","            # and out_features = number of classes = 10 (MNIST data 0-9)\n","            nn.Linear(in_features=84, out_features=num_classes)\n","        )\n","\n","    def forward(self, x):\n","        # apply feature extractor\n","        x = self._body(x)\n","        # flatten the output of conv layers\n","        # dimension should be batch_size * number_of weights_in_last conv_layer\n","        x = x.view(x.shape[0], -1)\n","        # apply classification head\n","        x = self._head(x)\n","        return x\n"]},{"cell_type":"markdown","source":["You will construct your model in this block. Make sure that you properly set the number of input channels!"],"metadata":{"id":"5HAPFcB5pLuN"}},{"cell_type":"code","source":["class MyModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self._body = nn.Sequential(\n","            # First convolution Layer(s?)\n","            # input size = (32, 32), output size = (32, 32)\n","            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=\"same\"),\n","            nn.BatchNorm2d(16),\n","\n","            # ReLU activation\n","            nn.ReLU(),\n","            # Max pool 2-d\n","            nn.MaxPool2d(kernel_size=2),\n","\n","            # Add in more convolution layers as needed, keep track of the sizes!\n","            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=0),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(inplace=True),\n","\n","            # output size should be = (32, 7, 7)\n","            )\n","        self._head = nn.Sequential(\n","            # First fully connected layer\n","            # in_features = total number of weights in last conv layer = 32 * 7 * 7\n","            nn.Linear(in_features=32 * 7 * 7, out_features=84),\n","\n","            # ReLU activation\n","            nn.ReLU(inplace=True),\n","\n","            # you can vary the number of fully connected layers and their sizes!\n","            # As long as you end up with 10 output features!\n","            nn.Linear(in_features=84, out_features=10)\n","        )\n","\n","    def forward(self, x):\n","        # apply feature extractor\n","        x = self._body(x)\n","        # flatten the output of conv layers\n","        # dimension should be batch_size * number_of weights_in_last conv_layer\n","        x = x.view(x.shape[0], -1)\n","        # apply classification head\n","        x = self._head(x)\n","\n","        return x"],"metadata":{"id":"zpakFpRopJwp","executionInfo":{"status":"ok","timestamp":1761597021155,"user_tz":300,"elapsed":39,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"execution_count":116,"outputs":[]},{"cell_type":"markdown","metadata":{"cellIdentifier":"mudioz4b2tfbum4z8m8hu","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"uwRKvkFRiIdo"},"source":["# <font style=\"color:blue\">2. Display the Network</font>"]},{"cell_type":"code","execution_count":117,"metadata":{"cellIdentifier":"lvt78d082ver9krn47nh1","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"B0ggL876iIdp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761597021167,"user_tz":300,"elapsed":11,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}},"outputId":"fa1f8371-5fe9-415d-bde7-ffc731571fc7"},"outputs":[{"output_type":"stream","name":"stdout","text":["MyModel(\n","  (_body): Sequential(\n","    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n","    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (6): ReLU(inplace=True)\n","    (7): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (9): ReLU(inplace=True)\n","    (10): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2))\n","    (11): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (12): ReLU(inplace=True)\n","    (13): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (14): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (15): ReLU(inplace=True)\n","  )\n","  (_head): Sequential(\n","    (0): Linear(in_features=1568, out_features=84, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Linear(in_features=84, out_features=10, bias=True)\n","  )\n",")\n"]}],"source":["my_model = MyModel()\n","print(my_model)"]},{"cell_type":"code","execution_count":118,"metadata":{"cellIdentifier":"45wz6xqxexnvkofd2cmp1j","deletable":false,"editable":false,"nbgrader":{"grade":true,"grade_id":"Conv2d","locked":true,"points":"3","solution":false},"id":"xgXHLkKniIdq","executionInfo":{"status":"ok","timestamp":1761597021168,"user_tz":300,"elapsed":0,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":["###\n","### AUTOGRADER TEST - DO NOT REMOVE\n","###\n"]},{"cell_type":"code","execution_count":119,"metadata":{"cellIdentifier":"gl49j5fysip2lruz7q4a76","deletable":false,"editable":false,"nbgrader":{"grade":true,"grade_id":"BatchNorm2d","locked":true,"points":"2","solution":false},"id":"Fshic515iIdq","executionInfo":{"status":"ok","timestamp":1761597021170,"user_tz":300,"elapsed":1,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":["###\n","### AUTOGRADER TEST - DO NOT REMOVE\n","###\n"]},{"cell_type":"code","execution_count":120,"metadata":{"cellIdentifier":"frob7e4vwdn9z5ufcdj5v5","deletable":false,"editable":false,"nbgrader":{"grade":true,"grade_id":"ReLU","locked":true,"points":"2","solution":false},"id":"56VGH_TmiIdq","executionInfo":{"status":"ok","timestamp":1761597021171,"user_tz":300,"elapsed":1,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":["###\n","### AUTOGRADER TEST - DO NOT REMOVE\n","###\n"]},{"cell_type":"code","execution_count":121,"metadata":{"cellIdentifier":"zh8x5bdcucn3g3knokhem","deletable":false,"editable":false,"nbgrader":{"grade":true,"grade_id":"input-output","locked":true,"points":"3","solution":false},"id":"vc2wzxu_iIdr","executionInfo":{"status":"ok","timestamp":1761597021172,"user_tz":300,"elapsed":1,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":["###\n","### AUTOGRADER TEST - DO NOT REMOVE\n","###\n"]},{"cell_type":"markdown","metadata":{"cellIdentifier":"gkzno8yk4lmvr9ssru1t","deletable":false,"editable":false,"lines_to_next_cell":2,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"Fzv-dxBAiIdr"},"source":["# <font style=\"color:blue\">3. Find Mean and STD of CIFAR10 Data [5 Points]</font>\n","\n","Function **`get_mean_std_train_data`** should `return` `mean` and `std` of training data. You can refer to the code used in the previous section for finding the mean and std of the training data.\n","\n"]},{"cell_type":"code","execution_count":122,"metadata":{"cellIdentifier":"kch1j79wzj3pbk3zg56yy","nbgrader":{"grade":false,"locked":false,"solution":false},"id":"UT39Oj5hiIdr","executionInfo":{"status":"ok","timestamp":1761597021173,"user_tz":300,"elapsed":1,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":["def get_mean_std_train_data(data_root):\n","\n","    train_transform = transforms.Compose([transforms.ToTensor()])\n","    train_set = datasets.CIFAR10(root=data_root, train=True, download=True, transform=train_transform)\n","\n","    # return mean (numpy.ndarray) and std (numpy.ndarray)\n","    mean = np.array([0.0, 0.0, 0.0])\n","    mean2 = np.array([0.0, 0.0, 0.0])\n","    count = 0\n","    for i in train_set:\n","      m = torch.mean(i[0], (1,2))\n","      m2 = torch.mean(i[0]*i[0], (1,2))\n","      mean += m.numpy()\n","      mean2 += m2.numpy()\n","      count +=1\n","    mean /= count\n","    std = np.sqrt(mean2 / count - mean * mean)\n","\n","\n","    return mean, std"]},{"cell_type":"code","execution_count":123,"metadata":{"cellIdentifier":"tb7y47hwwrjq6xrhzrin","deletable":false,"editable":false,"lines_to_next_cell":2,"nbgrader":{"grade":true,"grade_id":"mean","locked":true,"points":"2","solution":false},"id":"cYKvQMwRiIdr","executionInfo":{"status":"ok","timestamp":1761597021174,"user_tz":300,"elapsed":1,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":["###\n","### AUTOGRADER TEST - DO NOT REMOVE\n","###\n"]},{"cell_type":"code","execution_count":124,"metadata":{"cellIdentifier":"wyqatizw4cgxuiov0sr2d","deletable":false,"editable":false,"lines_to_next_cell":2,"nbgrader":{"grade":true,"grade_id":"std","locked":true,"points":"3","solution":false},"id":"X5c-BWARiIdr","executionInfo":{"status":"ok","timestamp":1761597021175,"user_tz":300,"elapsed":1,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":["###\n","### AUTOGRADER TEST - DO NOT REMOVE\n","###\n"]},{"cell_type":"code","execution_count":125,"metadata":{"cellIdentifier":"jmtupetc6cwe0x4vw18d","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"k0PtlhcMiIdr","executionInfo":{"status":"ok","timestamp":1761597021176,"user_tz":300,"elapsed":1,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":["def get_data(batch_size, data_root, num_workers=1):\n","\n","\n","    try:\n","        mean, std = get_mean_std_train_data(data_root)\n","        assert len(mean) == len(std) == 3\n","    except:\n","        mean = np.array([0.5, 0.5, 0.5])\n","        std = np.array([0.5, 0.5, 0.5])\n","\n","\n","    train_test_transforms = transforms.Compose([\n","        # this re-scale image tensor values between 0-1. image_tensor /= 255\n","        transforms.ToTensor(),\n","        # subtract mean and divide by variance.\n","        transforms.Normalize(mean, std)\n","    ])\n","\n","    # train dataloader\n","    train_loader = torch.utils.data.DataLoader(\n","        datasets.CIFAR10(root=data_root, train=True, download=True, transform=train_test_transforms),\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=num_workers,\n","        pin_memory = True\n","    )\n","\n","    # test dataloader\n","    test_loader = torch.utils.data.DataLoader(\n","        datasets.CIFAR10(root=data_root, train=False, download=True, transform=train_test_transforms),\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=num_workers,\n","        pin_memory = True\n","    )\n","    return train_loader, test_loader"]},{"cell_type":"markdown","metadata":{"cellIdentifier":"r88idgtj3p8xu75fz4kvlb","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"VelIMvhpiIds"},"source":["# <font style=\"color:blue\">4. System Configuration</font>\n"]},{"cell_type":"code","source":[],"metadata":{"id":"19AEaaZ6iURn","executionInfo":{"status":"ok","timestamp":1761597021177,"user_tz":300,"elapsed":1,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"execution_count":125,"outputs":[]},{"cell_type":"code","execution_count":126,"metadata":{"cellIdentifier":"gejc24rxqbpla939nku3lg","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"mtzM7nfliIds","executionInfo":{"status":"ok","timestamp":1761597021178,"user_tz":300,"elapsed":0,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":["@dataclass\n","class SystemConfiguration:\n","    '''\n","    Describes the common system setting needed for reproducible training\n","    '''\n","    seed: int = 42  # seed number to set the state of all random number generators\n","    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\n","    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)"]},{"cell_type":"markdown","metadata":{"cellIdentifier":"wqzg5wnztvxnl2x79jvy","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"efXyUPQqiIds"},"source":["# <font style=\"color:blue\">5. Training Configuration [15 Points]</font>\n","All training parameters are defined here. So,\n","This is where you can improve your accuracy, apart from improving the architecture.\n","\n","Here are a few hints on how you can improve the accuracy:\n","- Train for longer duration\n","- Try with different learning rate\n","\n","**You need to achieve 75% accuracy in order to get full marks for this part.**\n","\n","**You will see the effect of these changes when you run Step 11**"]},{"cell_type":"code","execution_count":127,"metadata":{"cellIdentifier":"aghiky90dpbiv8zwmmqg9d","nbgrader":{"grade":false,"locked":false,"solution":false},"id":"nPX9uHHhiIds","executionInfo":{"status":"ok","timestamp":1761597021205,"user_tz":300,"elapsed":26,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":["@dataclass\n","class TrainingConfiguration:\n","    '''\n","    Describes configuration of the training process\n","    '''\n","\n","\n","    log_interval: int = 1000  # how many batches to wait between logging training status\n","    test_interval: int = 1  # how many epochs to wait before another test. Set to 1 to get val loss at each epoch\n","    data_root: str = \"../data\"  # folder to save data\n","    num_workers: int = 10  # number of concurrent processes using to prepare data\n","    device: str = 'cuda'  # device to use for training.\n","    # update changed parameters in below coding block.\n","    # Please do not change \"data_root\"\n","    batch_size: int = 8  # amount of data to pass through the network at each forward-backward iteration\n","    epochs_count: int = 12  # number of times the whole dataset will be passed through the network\n","    learning_rate: float = 0.005  # determines the speed of network's weights update"]},{"cell_type":"markdown","metadata":{"cellIdentifier":"hhgp8qss2rcnae9zdzbw2b","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"rjhKIBeSiIdt"},"source":["# <font style=\"color:blue\">6. System Setup</font>\n"]},{"cell_type":"code","execution_count":128,"metadata":{"cellIdentifier":"lxiahxmavindgswiy99dt","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"292R-GrkiIdt","executionInfo":{"status":"ok","timestamp":1761597021214,"user_tz":300,"elapsed":1,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":["def setup_system(system_config: SystemConfiguration) -> None:\n","    torch.manual_seed(system_config.seed)\n","    if torch.cuda.is_available():\n","        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n","        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic"]},{"cell_type":"markdown","metadata":{"cellIdentifier":"n87gzfiems5sx6r347ue","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"7sP1ug6BiIdt"},"source":["# <font style=\"color:blue\">7. Training</font>\n","We are familiar with the training pipeline used in PyTorch. The following steps are performed in the code below:\n","\n","1. Send the data to the required device ( CPU/GPU )\n","1. Make a forward pass using the forward method.\n","1. Find the loss using the Cross_Entropy function.\n","1. Find the gradients using the backward function.\n","1. Update the weights using the optimizer.\n","1. Find the accuracy of the model\n","\n","Repeat the above for the specified number of epochs."]},{"cell_type":"code","execution_count":129,"metadata":{"cellIdentifier":"quivu39z5nsqsx9u6rxo","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"07v_UkAKiIdu","executionInfo":{"status":"ok","timestamp":1761597021215,"user_tz":300,"elapsed":0,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":["def train(\n","    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n","    train_loader: torch.utils.data.DataLoader, epoch_idx: int\n",") -> None:\n","\n","    # change model in training mood\n","    model.train()\n","\n","    # to get batch loss\n","    batch_loss = np.array([])\n","\n","    # to get batch accuracy\n","    batch_acc = np.array([])\n","\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","\n","        # clone target\n","        indx_target = target.clone()\n","        # send data to device (its is medatory if GPU has to be used)\n","        data = data.to(train_config.device)\n","        # send target to device\n","        target = target.to(train_config.device)\n","\n","        # reset parameters gradient to zero\n","        optimizer.zero_grad()\n","\n","        # forward pass to the model\n","        output = model(data)\n","\n","        # cross entropy loss\n","        loss = F.cross_entropy(output, target)\n","\n","        # find gradients w.r.t training parameters\n","        loss.backward()\n","        # Update parameters using gardients\n","        optimizer.step()\n","\n","        batch_loss = np.append(batch_loss, [loss.item()])\n","\n","        # Score to probability using softmax\n","        prob = F.softmax(output, dim=1)\n","\n","        # get the index of the max probability\n","        pred = prob.data.max(dim=1)[1]\n","\n","        # correct prediction\n","        correct = pred.cpu().eq(indx_target).sum()\n","\n","        # accuracy\n","        acc = float(correct) / float(len(data))\n","\n","        batch_acc = np.append(batch_acc, [acc])\n","\n","        if batch_idx % train_config.log_interval == 0 and batch_idx > 0:\n","            print(\n","                'Train Epoch: {} [{}/{}] Loss: {:.6f} Acc: {:.4f}'.format(\n","                    epoch_idx, batch_idx * len(data), len(train_loader.dataset), loss.item(), acc\n","                )\n","            )\n","\n","    epoch_loss = batch_loss.mean()\n","    epoch_acc = batch_acc.mean()\n","    return epoch_loss, epoch_acc"]},{"cell_type":"markdown","metadata":{"cellIdentifier":"xcy66pi4ycnjkd84kl4zm","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"xGt2RE_oiIdu"},"source":["# <font style=\"color:blue\">8. Validation</font>\n","\n","After every few epochs **`validation`** will be called with the `trained model` and `test_loader` to get validation loss and accuracy."]},{"cell_type":"code","execution_count":130,"metadata":{"cellIdentifier":"4lbcau1ndx65er2dv92xs","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"LU8bYPk6iIdu","executionInfo":{"status":"ok","timestamp":1761597021216,"user_tz":300,"elapsed":0,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":["def validate(\n","    train_config: TrainingConfiguration,\n","    model: nn.Module,\n","    test_loader: torch.utils.data.DataLoader,\n",") -> float:\n","    model.eval()\n","    test_loss = 0\n","    count_corect_predictions = 0\n","    for data, target in test_loader:\n","        indx_target = target.clone()\n","        data = data.to(train_config.device)\n","\n","        target = target.to(train_config.device)\n","\n","        output = model(data)\n","        # add loss for each mini batch\n","        test_loss += F.cross_entropy(output, target).item()\n","\n","        # Score to probability using softmax\n","        prob = F.softmax(output, dim=1)\n","\n","        # get the index of the max probability\n","        pred = prob.data.max(dim=1)[1]\n","\n","        # add correct prediction count\n","        count_corect_predictions += pred.cpu().eq(indx_target).sum()\n","\n","    # average over number of mini-batches\n","    test_loss = test_loss / len(test_loader)\n","\n","    # average over number of dataset\n","    accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n","\n","    print(\n","        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","            test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n","        )\n","    )\n","    return test_loss, accuracy/100.0"]},{"cell_type":"markdown","metadata":{"cellIdentifier":"2ysvxjbn5batn3vlixqskk","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"ZrDtHJzIiIdu"},"source":["# <font style=\"color:blue\">9. Saving the Model</font>"]},{"cell_type":"code","execution_count":131,"metadata":{"cellIdentifier":"cigbyr07v3ryd1oboyy97f","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"ytM5nHqPiIdu","executionInfo":{"status":"ok","timestamp":1761597021217,"user_tz":300,"elapsed":0,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":["def save_model(model, device, model_dir='models', model_file_name='cifar10_cnn_model.pt'):\n","\n","\n","    if not os.path.exists(model_dir):\n","        os.makedirs(model_dir)\n","\n","    model_path = os.path.join(model_dir, model_file_name)\n","\n","    # make sure you transfer the model to cpu.\n","    if device == 'cuda':\n","        model.to('cpu')\n","\n","    # save the state_dict\n","    if int(torch.__version__.split('.')[1]) >= 6:\n","        torch.save(model.state_dict(), model_path, _use_new_zipfile_serialization=False)\n","\n","    else:\n","        torch.save(model.state_dict(), model_path)\n","\n","    if device == 'cuda':\n","        model.to('cuda')\n","\n","    return"]},{"cell_type":"markdown","metadata":{"cellIdentifier":"1wru951nwhhhcr3x60oqvi","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"P29OJtdOiIdv"},"source":["# <font style=\"color:blue\">10. Main</font>\n","\n","In this section of code, we use the configuration parameters defined above and start the training. Here are the important actions being taken in the code below:\n","\n","1. Set up system parameters like CPU/GPU, number of threads etc\n","1. Load the data using dataloaders\n","1. Create an instance of the LeNet model\n","1. Specify optimizer to use.\n","1. Set up variables to track loss and accuracy and start training.\n","1. If loss decreases, saving the model"]},{"cell_type":"code","execution_count":132,"metadata":{"cellIdentifier":"vw2o5y02w699wsjpdeyeot","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"WflJgjQMiIdv","executionInfo":{"status":"ok","timestamp":1761597021219,"user_tz":300,"elapsed":1,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":["def main(system_configuration=SystemConfiguration(), training_configuration=TrainingConfiguration()):\n","\n","    # system configuration\n","    setup_system(system_configuration)\n","\n","    # batch size\n","    batch_size_to_set = training_configuration.batch_size\n","    # num_workers\n","    num_workers_to_set = training_configuration.num_workers\n","    # epochs\n","    epoch_num_to_set = training_configuration.epochs_count\n","\n","    # if GPU is available use training config,\n","    # else lowers batch_size, num_workers and epochs count\n","    if torch.cuda.is_available():\n","        device = \"cuda\"\n","    else:\n","        device = \"cpu\"\n","        num_workers_to_set = 2\n","\n","    # data loader\n","    train_loader, test_loader = get_data(\n","        batch_size=training_configuration.batch_size,\n","        data_root=training_configuration.data_root,\n","        num_workers=num_workers_to_set\n","    )\n","\n","    # Update training configuration\n","    training_configuration = TrainingConfiguration(\n","        device=device,\n","        num_workers=num_workers_to_set\n","    )\n","\n","    # initiate model\n","    model = MyModel()\n","\n","    # send model to device (GPU/CPU)\n","    model.to(training_configuration.device)\n","\n","    # optimizer\n","    optimizer = optim.SGD(\n","        model.parameters(),\n","        lr=training_configuration.learning_rate\n","    )\n","\n","    # Learning rate scheduler\n","    # Decrease learning rate by a factor of 0.1 every 10 epochs\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","\n","\n","    best_loss = torch.tensor(np.inf)\n","    best_accuracy = torch.tensor(0)\n","\n","    # epoch train/test loss\n","    epoch_train_loss = np.array([])\n","    epoch_test_loss = np.array([])\n","\n","    # epch train/test accuracy\n","    epoch_train_acc = np.array([])\n","    epoch_test_acc = np.array([])\n","\n","    # trainig time measurement\n","    t_begin = time.time()\n","    for epoch in range(training_configuration.epochs_count):\n","\n","        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch)\n","\n","        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n","\n","        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n","\n","        elapsed_time = time.time() - t_begin\n","        speed_epoch = elapsed_time / (epoch + 1)\n","        speed_batch = speed_epoch / len(train_loader)\n","        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n","\n","        print(\n","            \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n","                elapsed_time, speed_epoch, speed_batch, eta\n","            )\n","        )\n","\n","        if epoch % training_configuration.test_interval == 0:\n","            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n","\n","            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n","\n","            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n","\n","            if current_loss < best_loss:\n","                best_loss = current_loss\n","\n","            if current_accuracy > best_accuracy:\n","                best_accuracy = current_accuracy\n","                print('Accuracy improved, saving the model.\\n')\n","                save_model(model, device)\n","\n","        # Step the learning rate scheduler\n","        scheduler.step()\n","\n","\n","    print(\"Total time: {:.2f}, Best Loss: {:.3f}, Best Accuracy: {:.3f}\".format(time.time() - t_begin, best_loss,\n","                                                                                best_accuracy))\n","\n","    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc"]},{"cell_type":"code","execution_count":133,"metadata":{"cellIdentifier":"dxj2uu63inbsx14oqdd9t","deletable":false,"editable":false,"nbgrader":{"grade":true,"grade_id":"Accuracy","locked":true,"points":"15","solution":false},"id":"0AACikEtiIdv","executionInfo":{"status":"ok","timestamp":1761597021220,"user_tz":300,"elapsed":1,"user":{"displayName":"Jack Krause","userId":"14792945510276893568"}}},"outputs":[],"source":["# @title\n","###\n","### AUTOGRADER TEST - DO NOT REMOVE\n","###\n"]},{"cell_type":"markdown","metadata":{"cellIdentifier":"k16izvw1m4egy23zhya6k","id":"GvvkraoaiIdw"},"source":["# <font style=\"color:blue\">Step 11: Start Training</font>\n","This is where you start the training. You may see that the training does not converge or does not give a good accuracy. You need to change\n","- In Step 1: the network architecture and add a few more layers or more nodes to the already existing layers\n","- In Step 5: training parameters such as learning rate or batch_size or epochs so that the network converges or run the network for longer so that it gets more time to fit the data\n","\n","**You need to make sure that the accuracy at the end is at least 75%.**"]},{"cell_type":"code","execution_count":null,"metadata":{"cellIdentifier":"emdjxj8b7al44hegpyrwg9","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"scrolled":true,"id":"B9nWN_xXiIdw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"513187c1-55fe-4e67-bb0e-434c6e4d356e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train Epoch: 0 [8000/50000] Loss: 1.938373 Acc: 0.2500\n","Train Epoch: 0 [16000/50000] Loss: 1.685186 Acc: 0.5000\n","Train Epoch: 0 [24000/50000] Loss: 0.823848 Acc: 0.6250\n","Train Epoch: 0 [32000/50000] Loss: 1.510882 Acc: 0.5000\n","Train Epoch: 0 [40000/50000] Loss: 2.007112 Acc: 0.2500\n","Train Epoch: 0 [48000/50000] Loss: 1.655704 Acc: 0.3750\n","Elapsed 26.87s, 26.87 s/epoch, 0.00 s/batch, ets 295.52s\n","\n","Test set: Average loss: 1.1301, Accuracy: 5965/10000 (60%)\n","\n","Accuracy improved, saving the model.\n","\n","Train Epoch: 1 [8000/50000] Loss: 1.004681 Acc: 0.6250\n","Train Epoch: 1 [16000/50000] Loss: 0.532625 Acc: 0.8750\n","Train Epoch: 1 [24000/50000] Loss: 0.871926 Acc: 0.7500\n","Train Epoch: 1 [32000/50000] Loss: 1.104988 Acc: 0.6250\n","Train Epoch: 1 [40000/50000] Loss: 1.214625 Acc: 0.5000\n","Train Epoch: 1 [48000/50000] Loss: 1.804792 Acc: 0.6250\n","Elapsed 57.55s, 28.77 s/epoch, 0.00 s/batch, ets 287.74s\n","\n","Test set: Average loss: 1.0019, Accuracy: 6503/10000 (65%)\n","\n","Accuracy improved, saving the model.\n","\n","Train Epoch: 2 [8000/50000] Loss: 0.652563 Acc: 0.8750\n","Train Epoch: 2 [16000/50000] Loss: 1.050028 Acc: 0.7500\n","Train Epoch: 2 [24000/50000] Loss: 1.168367 Acc: 0.5000\n","Train Epoch: 2 [32000/50000] Loss: 0.649858 Acc: 0.6250\n","Train Epoch: 2 [40000/50000] Loss: 1.102505 Acc: 0.6250\n","Train Epoch: 2 [48000/50000] Loss: 1.126378 Acc: 0.7500\n","Elapsed 88.49s, 29.50 s/epoch, 0.00 s/batch, ets 265.46s\n","\n","Test set: Average loss: 0.9193, Accuracy: 6831/10000 (68%)\n","\n","Accuracy improved, saving the model.\n","\n","Train Epoch: 3 [8000/50000] Loss: 1.161504 Acc: 0.6250\n","Train Epoch: 3 [16000/50000] Loss: 0.160352 Acc: 1.0000\n","Train Epoch: 3 [24000/50000] Loss: 1.436326 Acc: 0.5000\n","Train Epoch: 3 [32000/50000] Loss: 0.647997 Acc: 0.6250\n","Train Epoch: 3 [40000/50000] Loss: 0.734697 Acc: 0.6250\n","Train Epoch: 3 [48000/50000] Loss: 0.371245 Acc: 0.8750\n","Elapsed 118.96s, 29.74 s/epoch, 0.00 s/batch, ets 237.91s\n","\n","Test set: Average loss: 0.8296, Accuracy: 7113/10000 (71%)\n","\n","Accuracy improved, saving the model.\n","\n","Train Epoch: 4 [8000/50000] Loss: 0.908215 Acc: 0.7500\n","Train Epoch: 4 [16000/50000] Loss: 0.720509 Acc: 0.7500\n","Train Epoch: 4 [24000/50000] Loss: 0.681694 Acc: 0.7500\n","Train Epoch: 4 [32000/50000] Loss: 0.544467 Acc: 0.7500\n","Train Epoch: 4 [40000/50000] Loss: 1.274185 Acc: 0.6250\n","Train Epoch: 4 [48000/50000] Loss: 0.074409 Acc: 1.0000\n","Elapsed 149.89s, 29.98 s/epoch, 0.00 s/batch, ets 209.85s\n","\n","Test set: Average loss: 0.8426, Accuracy: 7069/10000 (71%)\n","\n","Train Epoch: 5 [8000/50000] Loss: 0.519445 Acc: 0.7500\n","Train Epoch: 5 [16000/50000] Loss: 1.627539 Acc: 0.5000\n","Train Epoch: 5 [24000/50000] Loss: 0.963925 Acc: 0.6250\n","Train Epoch: 5 [32000/50000] Loss: 1.069509 Acc: 0.6250\n","Train Epoch: 5 [40000/50000] Loss: 0.523100 Acc: 0.8750\n","Train Epoch: 5 [48000/50000] Loss: 1.369617 Acc: 0.6250\n","Elapsed 181.39s, 30.23 s/epoch, 0.00 s/batch, ets 181.39s\n","\n","Test set: Average loss: 0.7809, Accuracy: 7353/10000 (74%)\n","\n","Accuracy improved, saving the model.\n","\n","Train Epoch: 6 [8000/50000] Loss: 0.528765 Acc: 0.8750\n","Train Epoch: 6 [16000/50000] Loss: 0.472715 Acc: 0.8750\n","Train Epoch: 6 [24000/50000] Loss: 0.999760 Acc: 0.6250\n","Train Epoch: 6 [32000/50000] Loss: 0.584930 Acc: 0.8750\n","Train Epoch: 6 [40000/50000] Loss: 0.306857 Acc: 0.8750\n","Train Epoch: 6 [48000/50000] Loss: 0.549935 Acc: 0.6250\n","Elapsed 211.70s, 30.24 s/epoch, 0.00 s/batch, ets 151.22s\n","\n","Test set: Average loss: 0.8155, Accuracy: 7213/10000 (72%)\n","\n","Train Epoch: 7 [8000/50000] Loss: 0.740908 Acc: 0.8750\n","Train Epoch: 7 [16000/50000] Loss: 0.503196 Acc: 0.7500\n","Train Epoch: 7 [24000/50000] Loss: 0.943352 Acc: 0.6250\n","Train Epoch: 7 [32000/50000] Loss: 0.336135 Acc: 0.8750\n","Train Epoch: 7 [40000/50000] Loss: 0.955074 Acc: 0.5000\n","Train Epoch: 7 [48000/50000] Loss: 0.700873 Acc: 0.7500\n","Elapsed 242.41s, 30.30 s/epoch, 0.00 s/batch, ets 121.21s\n","\n","Test set: Average loss: 0.7721, Accuracy: 7364/10000 (74%)\n","\n","Accuracy improved, saving the model.\n","\n","Train Epoch: 8 [8000/50000] Loss: 0.357446 Acc: 0.8750\n","Train Epoch: 8 [16000/50000] Loss: 0.826473 Acc: 0.7500\n","Train Epoch: 8 [24000/50000] Loss: 0.600488 Acc: 0.7500\n","Train Epoch: 8 [32000/50000] Loss: 0.323095 Acc: 0.8750\n","Train Epoch: 8 [40000/50000] Loss: 0.480612 Acc: 0.7500\n","Train Epoch: 8 [48000/50000] Loss: 1.263097 Acc: 0.7500\n","Elapsed 273.00s, 30.33 s/epoch, 0.00 s/batch, ets 91.00s\n","\n","Test set: Average loss: 0.7980, Accuracy: 7319/10000 (73%)\n","\n","Train Epoch: 9 [8000/50000] Loss: 0.639388 Acc: 0.7500\n","Train Epoch: 9 [16000/50000] Loss: 0.467934 Acc: 0.8750\n","Train Epoch: 9 [24000/50000] Loss: 0.746820 Acc: 0.6250\n","Train Epoch: 9 [32000/50000] Loss: 0.450216 Acc: 0.7500\n","Train Epoch: 9 [40000/50000] Loss: 0.249307 Acc: 0.8750\n","Train Epoch: 9 [48000/50000] Loss: 0.300996 Acc: 0.8750\n","Elapsed 303.60s, 30.36 s/epoch, 0.00 s/batch, ets 60.72s\n","\n","Test set: Average loss: 0.7872, Accuracy: 7367/10000 (74%)\n","\n","Accuracy improved, saving the model.\n","\n","Train Epoch: 10 [8000/50000] Loss: 0.196744 Acc: 1.0000\n","Train Epoch: 10 [16000/50000] Loss: 0.435964 Acc: 0.8750\n","Train Epoch: 10 [24000/50000] Loss: 0.194835 Acc: 0.8750\n","Train Epoch: 10 [32000/50000] Loss: 0.349934 Acc: 0.8750\n","Train Epoch: 10 [40000/50000] Loss: 0.411240 Acc: 0.7500\n","Train Epoch: 10 [48000/50000] Loss: 0.452921 Acc: 0.7500\n","Elapsed 334.23s, 30.38 s/epoch, 0.00 s/batch, ets 30.38s\n","\n","Test set: Average loss: 0.7138, Accuracy: 7632/10000 (76%)\n","\n","Accuracy improved, saving the model.\n","\n","Train Epoch: 11 [8000/50000] Loss: 0.489329 Acc: 0.7500\n"]}],"source":["if required_training:\n","    model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc = main()"]},{"cell_type":"markdown","metadata":{"cellIdentifier":"126vblu0mtqaloks39h6ee","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"HXxOJiHwiIdw"},"source":["# <font style=\"color:blue\">12. Plot Loss</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellIdentifier":"cnwn12cbb8lr92qmo7bl1","deletable":false,"editable":false,"lines_to_next_cell":2,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"7CU4P9cViIdw"},"outputs":[],"source":["# Plot loss\n","plt.rcParams[\"figure.figsize\"] = (10, 6)\n","x = range(len(epoch_train_loss))\n","\n","\n","plt.figure\n","plt.plot(x, epoch_train_loss, color='r', label=\"train loss\")\n","plt.plot(x, epoch_test_loss, color='b', label=\"validation loss\")\n","plt.xlabel('epoch no.')\n","plt.ylabel('loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"cellIdentifier":"ym2dan5i35cem1ssyxjngd","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"bbVEE9sciIdw"},"source":["# <font style=\"color:blue\">13. Plot Accuracy</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellIdentifier":"jpdg4uoaizpwk0v6v01jo","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"EgbnYMIqiIdw"},"outputs":[],"source":["# Plot loss\n","plt.rcParams[\"figure.figsize\"] = (10, 6)\n","x = range(len(epoch_train_loss))\n","\n","\n","plt.figure\n","plt.plot(x, epoch_train_acc, color='r', label=\"train accuracy\")\n","plt.plot(x, epoch_test_acc, color='b', label=\"validation accuracy\")\n","plt.xlabel('epoch no.')\n","plt.ylabel('accuracy')\n","plt.legend(loc='center right')\n","plt.title('Training and Validation Accuracy')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"cellIdentifier":"6uchyp9ontkaujyhsohmh","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"LE143BpEiIdx"},"source":["# <font style=\"color:blue\">14. Loading the Model </font>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellIdentifier":"vsefgebcc9s0bfp04yglfji","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"O1Skrwn6iIdx"},"outputs":[],"source":["# initialize the model\n","cnn_model = MyModel()\n","\n","models = 'models'\n","\n","model_file_name = 'cifar10_cnn_model.pt'\n","\n","model_path = os.path.join(models, model_file_name)\n","\n","# loading the model and getting model parameters by using load_state_dict\n","cnn_model.load_state_dict(torch.load(model_path))"]},{"cell_type":"markdown","metadata":{"cellIdentifier":"pqlkwksntnelftcf3lzjr","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"siDolbdjiIdx"},"source":["# <font style=\"color:blue\">15. Model Prediction</font>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellIdentifier":"1fqx2j21g3c5jha4kt9g25","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"RUt7_L7HiIdx"},"outputs":[],"source":["def prediction(model, train_config, batch_input):\n","\n","    # send model to cpu/cuda according to your system configuration\n","    model.to(train_config.device)\n","\n","    # it is important to do model.eval() before prediction\n","    model.eval()\n","\n","    data = batch_input.to(train_config.device)\n","\n","    output = model(data)\n","\n","    # Score to probability using softmax\n","    prob = F.softmax(output, dim=1)\n","\n","    # get the max probability\n","    pred_prob = prob.data.max(dim=1)[0]\n","\n","    # get the index of the max probability\n","    pred_index = prob.data.max(dim=1)[1]\n","\n","    return pred_index.cpu().numpy(), pred_prob.cpu().numpy()"]},{"cell_type":"markdown","metadata":{"cellIdentifier":"47rdo58knyhzd3toqdyod","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"xqw9iE5riIdx"},"source":["# <font style=\"color:blue\">16. Perform Inference on sample images </font>\n","\n","For prediction, we need to transform the data in the same way as we have done during training."]},{"cell_type":"code","execution_count":null,"metadata":{"cellIdentifier":"s4n38qylnwrotqs43ou3md","deletable":false,"editable":false,"lines_to_next_cell":2,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"Vfcm5IpJiIdx"},"outputs":[],"source":["classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","batch_size = 5\n","train_config = TrainingConfiguration()\n","\n","if torch.cuda.is_available():\n","    train_config.device = \"cuda\"\n","else:\n","    train_config.device = \"cpu\"\n","\n","\n","\n","# load test data without image transformation\n","test = torch.utils.data.DataLoader(\n","    datasets.CIFAR10(root=train_config.data_root, train=False, download=False,\n","                   transform=transforms.functional.to_tensor),\n","    batch_size=batch_size,\n","    shuffle=False,\n","    num_workers=1\n","    )\n","\n","try:\n","    mean, std = get_mean_std_train_data(data_root)\n","    assert len(mean) == len(std) == 3\n","except:\n","    mean = (0.5, 0.5, 0.5)\n","    std = (0.5, 0.5, 0.5)\n","\n","# load testdata with image transformation\n","image_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std)\n","    ])\n","\n","test_trans = torch.utils.data.DataLoader(\n","    datasets.CIFAR10(root=train_config.data_root, train=False, download=False, transform=image_transforms),\n","    batch_size=batch_size,\n","    shuffle=False,\n","    num_workers=1\n","    )\n","\n","for data, _ in test_trans:\n","    # pass the loaded model\n","    pred, prob = prediction(cnn_model, train_config, data)\n","    break\n","\n","\n","plt.rcParams[\"figure.figsize\"] = (3, 3)\n","for images, label in test:\n","    for i, img in enumerate(images):\n","        img = transforms.functional.to_pil_image(img)\n","        plt.imshow(img)\n","        plt.gca().set_title('Pred: {0}({1:0.2}), Label: {2}'.format(classes[pred[i]], prob[i], classes[label[i]]))\n","        plt.show()\n","    break"]},{"cell_type":"code","source":[],"metadata":{"id":"YfiBxqzn2q6W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"cellIdentifier":"8wb3nr9wakbwztwyrbsvb","id":"ZfWM92CmiIdx"},"source":["# <font style=\"color:blue\">17. Analysis Questions [20 points]\n","QUestion 1: What was the best accuracy that you got on the test set? What approaches did you try to improve your results? Make a table of your original model parameters and record your results, then start changing your model to improve results by adding in layers and processing, different learning rates, training longer, etc. Comment on which changes gave the most performance improvements versus the size of the network. The optimizer is restricted to the standard SGD operator for this case.\n","\n","Question 2 What categories of images were identified most accurately? Which categories were confused with each other more often? Use your results  Speculate on why some categories of image were more difficult to work with.\n","\n","Question 3 Is the system overfit? Support your answer.\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"Oa_Kf1URzvOr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <font style=\"color:blue\">18. Quiz Questions (10 pts)\n","1.   The input image size is (28, 28, 3) (colored image with RGB channel), and there is a convolutional layer with 8 filters whose kernel size is (3, 3) with no bias. What is the total number of weight parameters in the convolutional layer?\n","2.   Which of the following operations have learnable parameters? ReLU, Convolution 2D, Max Pooling, Batch Norm?\n","3.   Consider the input\n","\\begin{bmatrix}\n","    X_{11}  & X_{12} & X_{13} \\\\X_{21}  & X_{22} & X_{23} \\\\X_{31}  & X_{32} & X_{33}\\end{bmatrix}\n","\n","and the given kernel as:\n","\\begin{bmatrix}\n","    K_{11}  & K_{12}\\\\K_{21}  & K_{22}\\end{bmatrix}\n","\n","In the convolution operation which of the listed inputs will contribute to the gradient update of weight $K_{22}$ without padding? $X_{12}, X_{13},X_{22},X_{32}$?\n","\n","4.   Which of the following statements is true regarding max pooling in a CNN?\n","\n","a.   Max pooling layers increase the depth of the feature maps.\n","\n","b.   Max pooling is differentiable, and its gradient can be easily computed during backpropagation.\n","\n","  c.  Max pooling is a technique used to normalize the feature maps.\n","\n","  d.  Max pooling can provide a form of translation invariance to the network.\n","\n","\n"],"metadata":{"id":"ksHZwclXu24f"}},{"cell_type":"markdown","metadata":{"cellIdentifier":"heokkq5v9j4f9cy0gofodn","deletable":false,"editable":false,"nbgrader":{"grade":false,"locked":true,"solution":false},"id":"0vp7qdhtiIdy"},"source":["# <font style=\"color:blue\">References</font>\n","\n","1. https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n","1. https://pytorch.org/tutorials/beginner/saving_loading_models.html"]},{"cell_type":"code","source":["## Github management commands\n","%%bash\n","git status\n","git add --all\n","git status\n","git commit -m \"updated MyModel; save current model architecture\"\n","git push -u origin dev_one\n","git status\n"],"metadata":{"id":"EFirQhpO-EmC"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}